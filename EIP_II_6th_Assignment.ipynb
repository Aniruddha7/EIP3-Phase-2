{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP II 6th Assignment",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniruddha7/EIP3-Phase-2/blob/master/EIP_II_6th_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bIwaMJIKYaZ",
        "colab_type": "code",
        "outputId": "477fd84d-b799-441d-9206-d3e37630e18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import numpy\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation, Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 3.3MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2sPFqbCKn8B",
        "colab_type": "code",
        "outputId": "56349f41-174c-4f96-c7a4-0eec6000c9c5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6fe3cb7-c876-4fc9-8c02-38b5632d6ce5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d6fe3cb7-c876-4fc9-8c02-38b5632d6ce5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving aw3.txt to aw3.txt\n",
            "User uploaded file \"aw3.txt\" with length 75182 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8ZXKvwKqWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.path.isfile(\"drive/My Drive/Alice.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iBP_Gd_K_lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "#import glob\n",
        "#file_path = glob.glob(\"/content/gdrive/My Drive/***.txt\")\n",
        "filename = \"aw3.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg3eaVBCLDuW",
        "colab_type": "code",
        "outputId": "ddff3b8e-aa34-43e3-fa6d-60a27c71e6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import string\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "raw_text = raw_text.translate(translator)\n",
        "print(raw_text[0:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever  you may copy it give it away or\n",
            "reuse it under the terms of the project gutenberg license included\n",
            "with this ebook or online at wwwgutenbergorg\n",
            "\n",
            "\n",
            "title alices adventures in wonderland\n",
            "\n",
            "author lewis carroll\n",
            "\n",
            "posting date june 25 2008 ebook 11\n",
            "release date march 1994\n",
            "last updated december 20 2011\n",
            "\n",
            "language english\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ali\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj8L8l03KtVz",
        "colab_type": "code",
        "outputId": "f3021712-bc70-466b-e6eb-2f3372a1c245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "model.save_weights('EIP Assignment 6-{epoch:070d}-{val_acc:.2f}.hdf5')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  69711\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 05:50:30.769488 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0728 05:50:30.805362 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 05:50:30.815164 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  69611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0728 05:50:31.123725 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0728 05:50:31.134711 139658645464960 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0728 05:50:31.420285 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 05:50:31.441719 139658645464960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0728 05:50:34.586884 139658645464960 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "69611/69611 [==============================] - 296s 4ms/step - loss: 2.7952\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.79525, saving model to weights-improvement-01-2.7952-bigger.hdf5\n",
            "Epoch 2/100\n",
            "69611/69611 [==============================] - 294s 4ms/step - loss: 2.5426\n",
            "\n",
            "Epoch 00002: loss improved from 2.79525 to 2.54262, saving model to weights-improvement-02-2.5426-bigger.hdf5\n",
            "Epoch 3/100\n",
            "69611/69611 [==============================] - 295s 4ms/step - loss: 2.3434\n",
            "\n",
            "Epoch 00003: loss improved from 2.54262 to 2.34344, saving model to weights-improvement-03-2.3434-bigger.hdf5\n",
            "Epoch 4/100\n",
            "69611/69611 [==============================] - 295s 4ms/step - loss: 2.1757\n",
            "\n",
            "Epoch 00004: loss improved from 2.34344 to 2.17571, saving model to weights-improvement-04-2.1757-bigger.hdf5\n",
            "Epoch 5/100\n",
            "69611/69611 [==============================] - 293s 4ms/step - loss: 2.0516\n",
            "\n",
            "Epoch 00005: loss improved from 2.17571 to 2.05162, saving model to weights-improvement-05-2.0516-bigger.hdf5\n",
            "Epoch 6/100\n",
            "69611/69611 [==============================] - 289s 4ms/step - loss: 1.9513\n",
            "\n",
            "Epoch 00006: loss improved from 2.05162 to 1.95129, saving model to weights-improvement-06-1.9513-bigger.hdf5\n",
            "Epoch 7/100\n",
            "69611/69611 [==============================] - 296s 4ms/step - loss: 1.8623\n",
            "\n",
            "Epoch 00007: loss improved from 1.95129 to 1.86228, saving model to weights-improvement-07-1.8623-bigger.hdf5\n",
            "Epoch 8/100\n",
            "69611/69611 [==============================] - 295s 4ms/step - loss: 1.7859\n",
            "\n",
            "Epoch 00008: loss improved from 1.86228 to 1.78586, saving model to weights-improvement-08-1.7859-bigger.hdf5\n",
            "Epoch 9/100\n",
            "69611/69611 [==============================] - 292s 4ms/step - loss: 1.7155\n",
            "\n",
            "Epoch 00009: loss improved from 1.78586 to 1.71554, saving model to weights-improvement-09-1.7155-bigger.hdf5\n",
            "Epoch 10/100\n",
            "69611/69611 [==============================] - 292s 4ms/step - loss: 1.6538\n",
            "\n",
            "Epoch 00010: loss improved from 1.71554 to 1.65383, saving model to weights-improvement-10-1.6538-bigger.hdf5\n",
            "Epoch 11/100\n",
            "69611/69611 [==============================] - 294s 4ms/step - loss: 1.5930\n",
            "\n",
            "Epoch 00011: loss improved from 1.65383 to 1.59300, saving model to weights-improvement-11-1.5930-bigger.hdf5\n",
            "Epoch 12/100\n",
            "69611/69611 [==============================] - 291s 4ms/step - loss: 1.5411\n",
            "\n",
            "Epoch 00012: loss improved from 1.59300 to 1.54107, saving model to weights-improvement-12-1.5411-bigger.hdf5\n",
            "Epoch 13/100\n",
            "69611/69611 [==============================] - 290s 4ms/step - loss: 1.4857\n",
            "\n",
            "Epoch 00013: loss improved from 1.54107 to 1.48571, saving model to weights-improvement-13-1.4857-bigger.hdf5\n",
            "Epoch 14/100\n",
            "69611/69611 [==============================] - 291s 4ms/step - loss: 1.4344\n",
            "\n",
            "Epoch 00014: loss improved from 1.48571 to 1.43442, saving model to weights-improvement-14-1.4344-bigger.hdf5\n",
            "Epoch 15/100\n",
            "69611/69611 [==============================] - 290s 4ms/step - loss: 1.3861\n",
            "\n",
            "Epoch 00015: loss improved from 1.43442 to 1.38610, saving model to weights-improvement-15-1.3861-bigger.hdf5\n",
            "Epoch 16/100\n",
            "69611/69611 [==============================] - 290s 4ms/step - loss: 1.3385\n",
            "\n",
            "Epoch 00016: loss improved from 1.38610 to 1.33848, saving model to weights-improvement-16-1.3385-bigger.hdf5\n",
            "Epoch 17/100\n",
            "69611/69611 [==============================] - 289s 4ms/step - loss: 1.2935\n",
            "\n",
            "Epoch 00017: loss improved from 1.33848 to 1.29347, saving model to weights-improvement-17-1.2935-bigger.hdf5\n",
            "Epoch 18/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 1.2515\n",
            "\n",
            "Epoch 00018: loss improved from 1.29347 to 1.25146, saving model to weights-improvement-18-1.2515-bigger.hdf5\n",
            "Epoch 19/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 1.2080\n",
            "\n",
            "Epoch 00019: loss improved from 1.25146 to 1.20803, saving model to weights-improvement-19-1.2080-bigger.hdf5\n",
            "Epoch 20/100\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 1.1710\n",
            "\n",
            "Epoch 00020: loss improved from 1.20803 to 1.17095, saving model to weights-improvement-20-1.1710-bigger.hdf5\n",
            "Epoch 21/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 1.1334\n",
            "\n",
            "Epoch 00021: loss improved from 1.17095 to 1.13338, saving model to weights-improvement-21-1.1334-bigger.hdf5\n",
            "Epoch 22/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 1.0961\n",
            "\n",
            "Epoch 00022: loss improved from 1.13338 to 1.09613, saving model to weights-improvement-22-1.0961-bigger.hdf5\n",
            "Epoch 23/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 1.0625\n",
            "\n",
            "Epoch 00023: loss improved from 1.09613 to 1.06250, saving model to weights-improvement-23-1.0625-bigger.hdf5\n",
            "Epoch 24/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 1.0283\n",
            "\n",
            "Epoch 00024: loss improved from 1.06250 to 1.02829, saving model to weights-improvement-24-1.0283-bigger.hdf5\n",
            "Epoch 25/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.9980\n",
            "\n",
            "Epoch 00025: loss improved from 1.02829 to 0.99796, saving model to weights-improvement-25-0.9980-bigger.hdf5\n",
            "Epoch 26/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.9683\n",
            "\n",
            "Epoch 00026: loss improved from 0.99796 to 0.96826, saving model to weights-improvement-26-0.9683-bigger.hdf5\n",
            "Epoch 27/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.9386\n",
            "\n",
            "Epoch 00027: loss improved from 0.96826 to 0.93864, saving model to weights-improvement-27-0.9386-bigger.hdf5\n",
            "Epoch 28/100\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.9082\n",
            "\n",
            "Epoch 00028: loss improved from 0.93864 to 0.90821, saving model to weights-improvement-28-0.9082-bigger.hdf5\n",
            "Epoch 29/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8865\n",
            "\n",
            "Epoch 00029: loss improved from 0.90821 to 0.88653, saving model to weights-improvement-29-0.8865-bigger.hdf5\n",
            "Epoch 30/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8597\n",
            "\n",
            "Epoch 00030: loss improved from 0.88653 to 0.85970, saving model to weights-improvement-30-0.8597-bigger.hdf5\n",
            "Epoch 31/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8411\n",
            "\n",
            "Epoch 00031: loss improved from 0.85970 to 0.84109, saving model to weights-improvement-31-0.8411-bigger.hdf5\n",
            "Epoch 32/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8142\n",
            "\n",
            "Epoch 00032: loss improved from 0.84109 to 0.81418, saving model to weights-improvement-32-0.8142-bigger.hdf5\n",
            "Epoch 33/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.7930\n",
            "\n",
            "Epoch 00033: loss improved from 0.81418 to 0.79302, saving model to weights-improvement-33-0.7930-bigger.hdf5\n",
            "Epoch 34/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.7810\n",
            "\n",
            "Epoch 00034: loss improved from 0.79302 to 0.78096, saving model to weights-improvement-34-0.7810-bigger.hdf5\n",
            "Epoch 35/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.7515\n",
            "\n",
            "Epoch 00035: loss improved from 0.78096 to 0.75149, saving model to weights-improvement-35-0.7515-bigger.hdf5\n",
            "Epoch 36/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.7376\n",
            "\n",
            "Epoch 00036: loss improved from 0.75149 to 0.73764, saving model to weights-improvement-36-0.7376-bigger.hdf5\n",
            "Epoch 37/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.7299\n",
            "\n",
            "Epoch 00037: loss improved from 0.73764 to 0.72992, saving model to weights-improvement-37-0.7299-bigger.hdf5\n",
            "Epoch 38/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.7052\n",
            "\n",
            "Epoch 00038: loss improved from 0.72992 to 0.70523, saving model to weights-improvement-38-0.7052-bigger.hdf5\n",
            "Epoch 39/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.6901\n",
            "\n",
            "Epoch 00039: loss improved from 0.70523 to 0.69012, saving model to weights-improvement-39-0.6901-bigger.hdf5\n",
            "Epoch 40/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.6755\n",
            "\n",
            "Epoch 00040: loss improved from 0.69012 to 0.67553, saving model to weights-improvement-40-0.6755-bigger.hdf5\n",
            "Epoch 41/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 0.6578\n",
            "\n",
            "Epoch 00041: loss improved from 0.67553 to 0.65777, saving model to weights-improvement-41-0.6578-bigger.hdf5\n",
            "Epoch 42/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 0.6451\n",
            "\n",
            "Epoch 00042: loss improved from 0.65777 to 0.64506, saving model to weights-improvement-42-0.6451-bigger.hdf5\n",
            "Epoch 43/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 0.7846\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.64506\n",
            "Epoch 44/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.9143\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.64506\n",
            "Epoch 45/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 2.8052\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.64506\n",
            "Epoch 46/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 2.7296\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.64506\n",
            "Epoch 47/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.6976\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.64506\n",
            "Epoch 48/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.6763\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.64506\n",
            "Epoch 49/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.6560\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.64506\n",
            "Epoch 50/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.6416\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.64506\n",
            "Epoch 51/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 2.6215\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.64506\n",
            "Epoch 52/100\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 2.6066\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.64506\n",
            "Epoch 53/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.5904\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.64506\n",
            "Epoch 54/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.5799\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.64506\n",
            "Epoch 55/100\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.6981\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.64506\n",
            "Epoch 56/100\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.2885\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.64506\n",
            "Epoch 57/100\n",
            "41600/69611 [================>.............] - ETA: 1:53 - loss: 2.1718Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ithZCkAyiRj8",
        "colab_type": "code",
        "outputId": "7c1ee63b-7566-486c-e819-8adea0c9d447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model.fit(X, y, epochs=44, batch_size=64, callbacks=callbacks_list)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 0.9861\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.64506\n",
            "Epoch 2/44\n",
            "69611/69611 [==============================] - 288s 4ms/step - loss: 0.9649\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.64506\n",
            "Epoch 3/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 0.9417\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.64506\n",
            "Epoch 4/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 0.9240\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.64506\n",
            "Epoch 5/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.8964\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.64506\n",
            "Epoch 6/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8779\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.64506\n",
            "Epoch 7/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 0.8629\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.64506\n",
            "Epoch 8/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.8389\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.64506\n",
            "Epoch 9/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 0.8207\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.64506\n",
            "Epoch 10/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 0.8046\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.64506\n",
            "Epoch 11/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 0.7912\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.64506\n",
            "Epoch 12/44\n",
            "69611/69611 [==============================] - 285s 4ms/step - loss: 0.7693\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.64506\n",
            "Epoch 13/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 0.7531\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.64506\n",
            "Epoch 14/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 0.7367\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.64506\n",
            "Epoch 15/44\n",
            "69611/69611 [==============================] - 285s 4ms/step - loss: 0.7230\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.64506\n",
            "Epoch 16/44\n",
            "69611/69611 [==============================] - 288s 4ms/step - loss: 0.7055\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.64506\n",
            "Epoch 17/44\n",
            "69611/69611 [==============================] - 285s 4ms/step - loss: 0.7002\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.64506\n",
            "Epoch 18/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 3.5848\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.64506\n",
            "Epoch 19/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 2.5680\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.64506\n",
            "Epoch 20/44\n",
            "69611/69611 [==============================] - 287s 4ms/step - loss: 2.0059\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.64506\n",
            "Epoch 21/44\n",
            "69611/69611 [==============================] - 291s 4ms/step - loss: 1.4854\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.64506\n",
            "Epoch 22/44\n",
            "69611/69611 [==============================] - 287s 4ms/step - loss: 1.0712\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.64506\n",
            "Epoch 23/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.9247\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.64506\n",
            "Epoch 24/44\n",
            "69611/69611 [==============================] - 285s 4ms/step - loss: 2.8294\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.64506\n",
            "Epoch 25/44\n",
            "69611/69611 [==============================] - 284s 4ms/step - loss: 2.7617\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.64506\n",
            "Epoch 26/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 2.7231\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.64506\n",
            "Epoch 27/44\n",
            "69611/69611 [==============================] - 286s 4ms/step - loss: 2.6953\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.64506\n",
            "Epoch 28/44\n",
            "69611/69611 [==============================] - 285s 4ms/step - loss: 2.6747\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.64506\n",
            "Epoch 29/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.6524\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.64506\n",
            "Epoch 30/44\n",
            "69611/69611 [==============================] - 283s 4ms/step - loss: 2.6321\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.64506\n",
            "Epoch 31/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.6046\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.64506\n",
            "Epoch 32/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.5775\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.64506\n",
            "Epoch 33/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.5551\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.64506\n",
            "Epoch 34/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.5468\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.64506\n",
            "Epoch 35/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.5138\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.64506\n",
            "Epoch 36/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.4929\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.64506\n",
            "Epoch 37/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.4735\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.64506\n",
            "Epoch 38/44\n",
            "69611/69611 [==============================] - 282s 4ms/step - loss: 2.4534\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.64506\n",
            "Epoch 39/44\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.4351\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.64506\n",
            "Epoch 40/44\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.4182\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.64506\n",
            "Epoch 41/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.4006\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.64506\n",
            "Epoch 42/44\n",
            "69611/69611 [==============================] - 281s 4ms/step - loss: 2.3829\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.64506\n",
            "Epoch 43/44\n",
            "69611/69611 [==============================] - 280s 4ms/step - loss: 2.3672\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.64506\n",
            "Epoch 44/44\n",
            "69611/69611 [==============================] - 279s 4ms/step - loss: 2.3513\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.64506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW2MQxCtIYRy",
        "colab_type": "code",
        "outputId": "b05f1b8f-c154-4fb1-a0e6-9f956edc17d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "import sys\n",
        "# load the network weights\n",
        "filename = \"EIP Assignment 6-{epoch:070d}-{val_acc:.2f}.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed: rabbit\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: rabbit\n",
            "\" little\n",
            "queer wont you\n",
            "\n",
            "not a bit said the caterpillar\n",
            "\n",
            "well perhaps your feelings may be different s \"\n",
            "oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}